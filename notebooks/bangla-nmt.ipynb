{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8556524,"sourceType":"datasetVersion","datasetId":5113293},{"sourceId":8593204,"sourceType":"datasetVersion","datasetId":5140374}],"dockerImageVersionId":30716,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-03T05:32:36.289751Z","iopub.execute_input":"2024-06-03T05:32:36.290137Z","iopub.status.idle":"2024-06-03T05:32:37.255431Z","shell.execute_reply.started":"2024-06-03T05:32:36.290107Z","shell.execute_reply":"2024-06-03T05:32:37.254453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import torch\n# torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-06-03T05:32:37.256928Z","iopub.execute_input":"2024-06-03T05:32:37.257296Z","iopub.status.idle":"2024-06-03T05:32:37.261193Z","shell.execute_reply.started":"2024-06-03T05:32:37.257271Z","shell.execute_reply":"2024-06-03T05:32:37.260297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install wandb -qqq\nimport wandb","metadata":{"execution":{"iopub.status.busy":"2024-06-03T05:32:37.262585Z","iopub.execute_input":"2024-06-03T05:32:37.262913Z","iopub.status.idle":"2024-06-03T05:32:51.29407Z","shell.execute_reply.started":"2024-06-03T05:32:37.262879Z","shell.execute_reply":"2024-06-03T05:32:51.293282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"wandb_api_key\")","metadata":{"execution":{"iopub.status.busy":"2024-06-03T05:32:51.296528Z","iopub.execute_input":"2024-06-03T05:32:51.29696Z","iopub.status.idle":"2024-06-03T05:32:51.610403Z","shell.execute_reply.started":"2024-06-03T05:32:51.296929Z","shell.execute_reply":"2024-06-03T05:32:51.609434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! wandb login $secret_value_0","metadata":{"execution":{"iopub.status.busy":"2024-06-03T05:32:51.611625Z","iopub.execute_input":"2024-06-03T05:32:51.611988Z","iopub.status.idle":"2024-06-03T05:32:54.476311Z","shell.execute_reply.started":"2024-06-03T05:32:51.611956Z","shell.execute_reply":"2024-06-03T05:32:54.47535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install transformers","metadata":{"execution":{"iopub.status.busy":"2024-06-03T05:32:54.477705Z","iopub.execute_input":"2024-06-03T05:32:54.478054Z","iopub.status.idle":"2024-06-03T05:33:06.622962Z","shell.execute_reply.started":"2024-06-03T05:32:54.478025Z","shell.execute_reply":"2024-06-03T05:33:06.622008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer\nfrom transformers import AutoTokenizer, MT5Model, TrainingArguments, Trainer, MT5ForConditionalGeneration, AutoModelForSeq2SeqLM\nimport torch\n\n# tokenizer = AutoTokenizer.from_pretrained(\"google/mt5-small\")\ntokenizer = AutoTokenizer.from_pretrained(\"csebuetnlp/banglat5_nmt_en_bn\", use_fast=False)\ntorch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel_name = \"csebuetnlp/banglat5_nmt_en_bn\" #\"google/mt5-small\"  # Adjust if using a pre-trained model\n# model = MT5ForConditionalGeneration.from_pretrained(model_name).to(torch_device)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(torch_device)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-03T05:33:06.624326Z","iopub.execute_input":"2024-06-03T05:33:06.624643Z","iopub.status.idle":"2024-06-03T05:33:34.381047Z","shell.execute_reply.started":"2024-06-03T05:33:06.624615Z","shell.execute_reply":"2024-06-03T05:33:34.380178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! pip install git+https://github.com/csebuetnlp/normalizer\nfrom normalizer import normalize","metadata":{"execution":{"iopub.status.busy":"2024-06-03T05:33:34.382176Z","iopub.execute_input":"2024-06-03T05:33:34.382833Z","iopub.status.idle":"2024-06-03T05:33:56.031941Z","shell.execute_reply.started":"2024-06-03T05:33:34.382806Z","shell.execute_reply":"2024-06-03T05:33:56.030783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv(\"/kaggle/input/pentabd-transliterated-dataset/train.csv\")\ndf_test = pd.read_csv(\"/kaggle/input/pentabd-transliterated-dataset/test.csv\")\ndf_val =  pd.read_csv(\"/kaggle/input/pentabd-transliterated-dataset/val.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-06-03T05:33:56.033747Z","iopub.execute_input":"2024-06-03T05:33:56.034509Z","iopub.status.idle":"2024-06-03T05:33:56.539477Z","shell.execute_reply.started":"2024-06-03T05:33:56.034469Z","shell.execute_reply":"2024-06-03T05:33:56.538681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train","metadata":{"execution":{"iopub.status.busy":"2024-06-03T05:33:56.544205Z","iopub.execute_input":"2024-06-03T05:33:56.544482Z","iopub.status.idle":"2024-06-03T05:33:56.568464Z","shell.execute_reply.started":"2024-06-03T05:33:56.544459Z","shell.execute_reply":"2024-06-03T05:33:56.56755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_train = df_train[:160]\n# df_val = df_val[:40]\n# df_test = df_test[:40]","metadata":{"execution":{"iopub.status.busy":"2024-06-03T05:33:56.569653Z","iopub.execute_input":"2024-06-03T05:33:56.570106Z","iopub.status.idle":"2024-06-03T05:33:56.57425Z","shell.execute_reply.started":"2024-06-03T05:33:56.570074Z","shell.execute_reply":"2024-06-03T05:33:56.573375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df_train.isna().sum())\nprint(df_test.isna().sum())\nprint(df_val.isna().sum())","metadata":{"execution":{"iopub.status.busy":"2024-06-03T05:33:56.575701Z","iopub.execute_input":"2024-06-03T05:33:56.576039Z","iopub.status.idle":"2024-06-03T05:33:56.604864Z","shell.execute_reply.started":"2024-06-03T05:33:56.57601Z","shell.execute_reply":"2024-06-03T05:33:56.603912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\n# Download necessary NLTK resources (may need internet connection)\nnltk.download('punkt')\nnltk.download('stopwords')\n\ndef clean_text(text, language='english'):\n    text = ''.join([char for char in text if char.isalnum() or char.isspace()])\n\n    # Convert to lowercase\n    text = text.lower()\n\n    # Remove stopwords (optional, adjust stopword list based on language)\n    stop_words = stopwords.words(language)\n    text = ' '.join([word for word in text.split() if word not in stop_words])\n\n    return text\n\n# Clean Banglish and Bengali text\n# df['Banglish_Clean'] = df['Banglish'].apply(clean_text)\n# df['Bengali_Clean'] = df['Bengali'].apply(clean_text, language='bengali')  # Specify Bengali for stopword removal\n\n# Normalization for Bengali text (replace with your desired normalization function)\ndef normalize_bengali(text):\n    normalized_text = normalize(text)\n    return normalized_text\n\ndf_train['normalized_bengali'] = df_train['text_bengali'].apply(normalize_bengali)\ndf_test['normalized_bengali'] = df_test['text_bengali'].apply(normalize_bengali)\ndf_val['normalized_bengali'] = df_val['text_bengali'].apply(normalize_bengali)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-03T05:33:56.607573Z","iopub.execute_input":"2024-06-03T05:33:56.608145Z","iopub.status.idle":"2024-06-03T05:34:04.613575Z","shell.execute_reply.started":"2024-06-03T05:33:56.608118Z","shell.execute_reply":"2024-06-03T05:34:04.612745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train[\"normalized_bengali\"][0]","metadata":{"execution":{"iopub.status.busy":"2024-06-03T05:34:04.614844Z","iopub.execute_input":"2024-06-03T05:34:04.615536Z","iopub.status.idle":"2024-06-03T05:34:04.62269Z","shell.execute_reply.started":"2024-06-03T05:34:04.615502Z","shell.execute_reply":"2024-06-03T05:34:04.621797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def find_max_length(df, column_name):\n    # Find the index of the text with the maximum length\n    max_length_index = df[column_name].str.len().idxmax()\n\n    # Get the text with the maximum length\n    max_length_text = df.loc[max_length_index, column_name]\n\n    # Print the maximum length and the corresponding text\n#     print(f\"Index of the text with maximum length: {max_length_index}\")\n#     print(f\"Maximum length: {len(max_length_text)}\")\n#     print(f\"Text with maximum length:\\n{max_length_text}\")\n    return len(max_length_text)\n\n# find_max_length(df_train, 'text_bengali')\n# print(df_train['text_bengali'][10454])","metadata":{"execution":{"iopub.status.busy":"2024-06-03T05:34:04.6238Z","iopub.execute_input":"2024-06-03T05:34:04.624104Z","iopub.status.idle":"2024-06-03T05:34:04.630792Z","shell.execute_reply.started":"2024-06-03T05:34:04.624069Z","shell.execute_reply":"2024-06-03T05:34:04.630034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import Dataset\ndef pad_truncate(df):\n    max_length = 200\n#     print(max_length)\n    bengali_tokenized = tokenizer(df['normalized_bengali'].tolist(), padding=\"max_length\", truncation=True)\n#     print(bengali_tokenized)\n#     max_length = find_max_length(df, 'text_transliterated')\n    banglish_tokenized = tokenizer(df['text_transliterated'].tolist(), padding=\"max_length\", truncation=True)\n\n    dataset = Dataset.from_dict({\n        \"input_ids\": banglish_tokenized[\"input_ids\"],\n        \"attention_mask\": banglish_tokenized[\"attention_mask\"],\n        \"labels\": bengali_tokenized[\"input_ids\"]  # Labels are target language tokens\n    })\n    \n    return dataset\n\ntrain_dataset = pad_truncate(df_train)\n# print(banglish, bengali)\n# pad_truncate(df_test)\n# pad_truncate(df_val)","metadata":{"execution":{"iopub.status.busy":"2024-06-03T05:34:04.632043Z","iopub.execute_input":"2024-06-03T05:34:04.632979Z","iopub.status.idle":"2024-06-03T05:34:30.477096Z","shell.execute_reply.started":"2024-06-03T05:34:04.632944Z","shell.execute_reply":"2024-06-03T05:34:30.476218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = pad_truncate(df_test)\nval_dataset = pad_truncate(df_val)","metadata":{"execution":{"iopub.status.busy":"2024-06-03T05:34:30.478244Z","iopub.execute_input":"2024-06-03T05:34:30.478542Z","iopub.status.idle":"2024-06-03T05:34:33.235576Z","shell.execute_reply.started":"2024-06-03T05:34:30.478518Z","shell.execute_reply":"2024-06-03T05:34:33.234793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n# training_args = TrainingArguments(\n#     output_dir=\"./mt5_banglish_bengali\",  # Output directory for checkpoints\n#     evaluation_strategy=\"steps\",\n#     overwrite_output_dir=True,  # Overwrite existing directory if it exists\n#     num_train_epochs=3,  # Adjust based on dataset size and desired accuracy\n#     per_device_train_batch_size=2,  # Adjust batch size based on GPU memory\n#     save_steps=50,  # Save model checkpoints every 10,000 steps\n#     save_total_limit=2,  # Keep only the most recent 2 checkpoints\n#     logging_steps=50,  # Log training progress every 500 steps\n#     fp16 = True,\n#     gradient_accumulation_steps = 6,\n#     load_best_model_at_end=True  # Load the best model based on validation metrics\n# )\n\nbatch_size = 4\nargs = Seq2SeqTrainingArguments(output_dir=\"weights\",\n                        evaluation_strategy=\"epoch\",\n                        save_strategy = \"epoch\",\n                        per_device_train_batch_size=batch_size,\n                        per_device_eval_batch_size=batch_size,\n                        learning_rate=2e-5,\n                        num_train_epochs=5,\n                        weight_decay=0.01,\n                        save_total_limit=3,\n                        predict_with_generate=True,\n                        fp16 = False,\n                        gradient_accumulation_steps = 6,\n                        save_steps = 50,\n                        logging_steps = 50,\n                        load_best_model_at_end=True,\n                        logging_dir=\"/logs\",\n                        report_to=\"wandb\")","metadata":{"execution":{"iopub.status.busy":"2024-06-03T05:34:33.236794Z","iopub.execute_input":"2024-06-03T05:34:33.237096Z","iopub.status.idle":"2024-06-03T05:34:33.277483Z","shell.execute_reply.started":"2024-06-03T05:34:33.237071Z","shell.execute_reply":"2024-06-03T05:34:33.276527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install bert-score\nfrom datasets import load_metric\n\n# Load the BERTScore metric\nbert_metric = load_metric('bertscore')","metadata":{"execution":{"iopub.status.busy":"2024-06-03T05:34:33.278815Z","iopub.execute_input":"2024-06-03T05:34:33.279113Z","iopub.status.idle":"2024-06-03T05:34:47.222144Z","shell.execute_reply.started":"2024-06-03T05:34:33.279088Z","shell.execute_reply":"2024-06-03T05:34:47.221187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_metrics(preds_and_labels):\n    preds, labels = preds_and_labels\n\n    # Decode the predictions and labels using the tokenizer, skipping special tokens\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n    \n    # Replace -100 (masked tokens) in labels with the pad token ID\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    \n    # Decode the labels using the tokenizer, skipping special tokens\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    \n    # Compute BERTScore using decoded predictions and labels\n    result = bert_metric.compute(predictions=decoded_preds, references=decoded_labels, lang='bn')\n    \n    # Return the BERTScore as a dictionary\n    return {\n      'BERT F1': np.mean(result['f1']),\n      'BERT Precision': np.mean(result['precision']),\n      'BERT Recall': np.mean(result['recall'])\n  }","metadata":{"execution":{"iopub.status.busy":"2024-06-03T05:34:47.223527Z","iopub.execute_input":"2024-06-03T05:34:47.224211Z","iopub.status.idle":"2024-06-03T05:34:47.231851Z","shell.execute_reply.started":"2024-06-03T05:34:47.224173Z","shell.execute_reply":"2024-06-03T05:34:47.230932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import DataCollatorForSeq2Seq\n\n# Instantiate a Seq2Seq model from the specified checkpoint\n\n# Define a data collator for Seq2Seq tasks\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=model)","metadata":{"execution":{"iopub.status.busy":"2024-06-03T05:34:47.232921Z","iopub.execute_input":"2024-06-03T05:34:47.233254Z","iopub.status.idle":"2024-06-03T05:34:47.245641Z","shell.execute_reply.started":"2024-06-03T05:34:47.233223Z","shell.execute_reply":"2024-06-03T05:34:47.244698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = Seq2SeqTrainer(\n    model=model,\n    args=args,\n    data_collator=data_collator,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    tokenizer=tokenizer,\n    compute_metrics = compute_metrics\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-03T05:34:47.247357Z","iopub.execute_input":"2024-06-03T05:34:47.247743Z","iopub.status.idle":"2024-06-03T05:34:47.284513Z","shell.execute_reply.started":"2024-06-03T05:34:47.247696Z","shell.execute_reply":"2024-06-03T05:34:47.283558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-06-03T05:34:47.285681Z","iopub.execute_input":"2024-06-03T05:34:47.28604Z","iopub.status.idle":"2024-06-03T06:20:14.878567Z","shell.execute_reply.started":"2024-06-03T05:34:47.286007Z","shell.execute_reply":"2024-06-03T06:20:14.876997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.evaluate()","metadata":{"execution":{"iopub.status.busy":"2024-06-03T06:20:14.880006Z","iopub.status.idle":"2024-06-03T06:20:14.880711Z","shell.execute_reply.started":"2024-06-03T06:20:14.880459Z","shell.execute_reply":"2024-06-03T06:20:14.880481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_output(input_sentence):\n    input_ids = tokenizer((input_sentence), return_tensors=\"pt\").input_ids.to(\"cuda\")\n    generated_tokens = model.generate(input_ids)\n    decoded_tokens = tokenizer.batch_decode(generated_tokens)[0]\n    decoded_tokens = normalize(decoded_tokens)\n\n    return decoded_tokens\n    \nprint(\"start\")   \ndf_test['predictions'] = df_test['text_transliterated'].apply(predict_output)\ndf_test.to_csv(\"banglaT5_nmt_en_bn_test.csv\", index=False)\nprint(\"complete\")              ","metadata":{"execution":{"iopub.status.busy":"2024-06-03T06:20:14.882415Z","iopub.status.idle":"2024-06-03T06:20:14.883181Z","shell.execute_reply.started":"2024-06-03T06:20:14.882885Z","shell.execute_reply":"2024-06-03T06:20:14.882909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_output(input_sentence):\n    input_ids = tokenizer((input_sentence), return_tensors=\"pt\").input_ids.to(\"cuda\")\n    generated_tokens = model.generate(input_ids)\n    decoded_tokens = tokenizer.batch_decode(generated_tokens)[0]\n    decoded_tokens = normalize(decoded_tokens)\n\n    return decoded_tokens\n    \nprint(\"start\")   \ndf_val['predictions'] = df_val['text_transliterated'].apply(predict_output)\ndf_val.to_csv(\"banglaT5_nmt_en_bn_val.csv\", index=False)\nprint(\"complete\")              ","metadata":{"execution":{"iopub.status.busy":"2024-06-03T06:20:14.884515Z","iopub.status.idle":"2024-06-03T06:20:14.884997Z","shell.execute_reply.started":"2024-06-03T06:20:14.884757Z","shell.execute_reply":"2024-06-03T06:20:14.884777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}