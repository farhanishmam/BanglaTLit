{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 8655714,
          "sourceType": "datasetVersion",
          "datasetId": 5113293
        }
      ],
      "dockerImageVersionId": 30732,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "script",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Automatic Back-Transliteration\n",
        "\n",
        "This notebook will get you started in running the automatic romanized Bangla back-transliteration model using the TB Encoder Enhance t5 model, which is the best performing model based on our experimental results."
      ],
      "metadata": {
        "id": "6BuL7PEjJhZx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing and Installing libraries"
      ],
      "metadata": {
        "id": "ba0zJhBYJKeQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gc\n",
        "import re\n",
        "import ast\n",
        "import sys\n",
        "import copy\n",
        "import json\n",
        "import time\n",
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import Parameter\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam, SGD, AdamW\n",
        "import torch.optim as optim\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, TrainingArguments\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from normalizer import normalize\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "w4fgvQ22Hutd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install git+https://github.com/csebuetnlp/normalizer"
      ],
      "metadata": {
        "id": "wIbz2PeIH01w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# device availability\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "metadata": {
        "id": "-Mmm2XNDHx6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuration Class"
      ],
      "metadata": {
        "id": "L1g676GLKMjf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CFG:\n",
        "    debug=False # want to debug or not\n",
        "    apex=True # for faster training\n",
        "    print_freq=1000\n",
        "    num_workers=4\n",
        "    model_name = 'csebuetnlp/banglat5'\n",
        "    encoder_name = \"aplycaebous/tb-BERT-fpt\"\n",
        "    epochs=8\n",
        "    learning_rate=2e-5\n",
        "    eps=1e-6\n",
        "    betas=(0.9, 0.999) # for adam optimizer\n",
        "    batch_size=6 # batch size\n",
        "    max_len=50\n",
        "    weight_decay=0.01 # for adam optimizer regulaization parameter\n",
        "    gradient_accumulation_steps=1\n",
        "    max_grad_norm=1000\n",
        "\n",
        "    seed=42 # seed no. for random initialization\n",
        "    train=True\n",
        "    fusion_mode = \"sum\" #\"concat\", \"sum\"\n",
        "    filtering_th = 70"
      ],
      "metadata": {
        "id": "NS5oRB83H8Wx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Tokenizer and Model"
      ],
      "metadata": {
        "id": "6RB4OYpGKRCs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tokenizer and model\n",
        "CFG.t5_tokenizer = T5Tokenizer.from_pretrained(CFG.model_name)\n",
        "CFG.t5_model = T5ForConditionalGeneration.from_pretrained(CFG.model_name)\n",
        "CFG.encoder_tokenizer = AutoTokenizer.from_pretrained(CFG.encoder_name)\n",
        "CFG.encoder_model = AutoModel.from_pretrained(CFG.encoder_name)\n",
        "CFG.device = device"
      ],
      "metadata": {
        "id": "Ya5uvauIIBCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reading the Dataset"
      ],
      "metadata": {
        "id": "u2Xaj1xCKUM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.read_csv(\"/kaggle/input/pentabd-transliteration-dataset-all-combined/train.csv\")\n",
        "df_test = pd.read_csv(\"/kaggle/input/pentabd-transliteration-dataset-all-combined/test.csv\")\n",
        "df_val =  pd.read_csv(\"/kaggle/input/pentabd-transliteration-dataset-all-combined/val.csv\")\n",
        "df_train.head()"
      ],
      "metadata": {
        "id": "3-ZlHgfuICP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple Data Analysis"
      ],
      "metadata": {
        "id": "GWo_poS5KWP-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count_greater = (df_train['word_count_transliterated'] > CFG.filtering_th).sum()\n",
        "print(\"Number of samples greater than 100 word count:\", count_greater)\n",
        "count_greater = (df_test['word_count_transliterated'] > CFG.filtering_th).sum()\n",
        "print(\"Number of samples greater than 100 word count:\", count_greater)\n",
        "count_greater = (df_val['word_count_transliterated'] > CFG.filtering_th).sum()\n",
        "print(\"Number of samples greater than 100 word count:\", count_greater)"
      ],
      "metadata": {
        "id": "iFweVHiNIgHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Splits"
      ],
      "metadata": {
        "id": "ocLFGflpKYpv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = df_train[df_train['word_count_transliterated'] <= CFG.filtering_th]\n",
        "df_val = df_val[df_val['word_count_transliterated'] <= CFG.filtering_th]\n",
        "df_test = df_test[df_test['word_count_transliterated'] <= CFG.filtering_th]\n",
        "if CFG.debug:\n",
        "    df_train = df_train[:160]\n",
        "    df_test = df_test[:40]\n",
        "    df_val = df_val[:40]"
      ],
      "metadata": {
        "id": "zl5JiettIhpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing"
      ],
      "metadata": {
        "id": "qb2pm5bLKbi-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalization for Bengali text (replace with your desired normalization function)\n",
        "def normalize_bengali(text):\n",
        "    normalized_text = normalize(text)\n",
        "    return normalized_text\n",
        "df_train['normalized_bengali'] = df_train['text_bengali'].apply(normalize_bengali)\n",
        "df_test['normalized_bengali'] = df_test['text_bengali'].apply(normalize_bengali)\n",
        "df_val['normalized_bengali'] = df_val['text_bengali'].apply(normalize_bengali)"
      ],
      "metadata": {
        "id": "Sz-XRRTyIpUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lengths = []\n",
        "tk0 = tqdm(df_train['text_transliterated'].fillna(\"\").values, total=len(df_train))\n",
        "for text in tk0:\n",
        "    length = len(CFG.t5_tokenizer(text, truncation=True, add_special_tokens=False)['input_ids'])\n",
        "    lengths.append(length)\n",
        "\n",
        "t5_tokenizer_max_len = max(lengths) + 5\n",
        "\n",
        "lengths = []\n",
        "for text in tk0:\n",
        "    length = len(CFG.encoder_tokenizer(text, truncation=True, add_special_tokens=False)['input_ids'])\n",
        "    lengths.append(length)\n",
        "\n",
        "\n",
        "encoder_tokenizer_max_len = max(lengths) + 5\n",
        "\n",
        "lengths = []\n",
        "tk1 = tqdm(df_train['normalized_bengali'].fillna(\"\").values, total=len(df_train))\n",
        "for text in tk1:\n",
        "    length = len(CFG.t5_tokenizer(text, truncation=True, add_special_tokens=False)['input_ids'])\n",
        "    lengths.append(length)\n",
        "\n",
        "target_max_len = max(lengths) + 5\n",
        "print(t5_tokenizer_max_len, encoder_tokenizer_max_len, target_max_len)\n",
        "\n",
        "CFG.max_len = max(t5_tokenizer_max_len, encoder_tokenizer_max_len, target_max_len)\n",
        "\n",
        "if CFG.max_len > 512:\n",
        "    CFG.max_len = 512\n",
        "\n",
        "print(CFG.max_len)"
      ],
      "metadata": {
        "id": "YUR7arK6I3Wk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the Dataset"
      ],
      "metadata": {
        "id": "ttlNPu9wKj6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a simple dataset for demonstration purposes\n",
        "class SimpleDataset(Dataset):\n",
        "    def __init__(self, t5_tokenizer, encoder_tokenizer, data, max_length=512):\n",
        "        self.t5_tokenizer = t5_tokenizer\n",
        "        self.encoder_tokenizer = encoder_tokenizer\n",
        "        self.transliterated_texts = data['text_transliterated'].tolist()\n",
        "        self.bangla_texts = data['text_bengali'].tolist()\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.bangla_texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input_text = self.transliterated_texts[idx]\n",
        "        target_text = self.bangla_texts[idx]\n",
        "        input_ids = self.t5_tokenizer(input_text, return_tensors='pt', padding='max_length', truncation=True, max_length=self.max_length).input_ids\n",
        "        encoder_inputs = self.encoder_tokenizer(input_text, return_tensors='pt', padding='max_length', truncation=True, max_length=self.max_length)\n",
        "        encoder_ids = encoder_inputs.input_ids\n",
        "        encoder_attn_mask = encoder_inputs.attention_mask\n",
        "        target_ids = self.t5_tokenizer(target_text, return_tensors='pt', padding='max_length', truncation=True, max_length=self.max_length).input_ids\n",
        "        return input_ids.squeeze(), encoder_ids.squeeze(), encoder_attn_mask.squeeze(), target_ids.squeeze()"
      ],
      "metadata": {
        "id": "I-cfhssYI7VV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare dataset and dataloader\n",
        "dataset = SimpleDataset(CFG.t5_tokenizer, CFG.encoder_tokenizer, df_train, max_length=CFG.max_len)\n",
        "train_dataloader = DataLoader(dataset, batch_size=CFG.batch_size, shuffle=True)\n",
        "\n",
        "dataset = SimpleDataset(CFG.t5_tokenizer, CFG.encoder_tokenizer, df_val, max_length=CFG.max_len)\n",
        "valid_dataloader = DataLoader(dataset, batch_size=CFG.batch_size, shuffle=False)\n",
        "\n",
        "dataset = SimpleDataset(CFG.t5_tokenizer, CFG.encoder_tokenizer, df_test, max_length=CFG.max_len)\n",
        "test_dataloader = DataLoader(dataset, batch_size=CFG.batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "Rr9vkcxEI80m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in train_dataloader:\n",
        "    print(i[0].shape)\n",
        "    print(i[1].shape)\n",
        "    print(i[2].shape)\n",
        "    break"
      ],
      "metadata": {
        "id": "7tBhm14TJA2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "RMuG6xKUKpfj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomModel(nn.Module):\n",
        "    def __init__(self, cfg, is_train = True):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.t5_model = self.cfg.t5_model\n",
        "        self.encoder_model = self.cfg.encoder_model\n",
        "        self.t5_tokenizer = self.cfg.t5_tokenizer\n",
        "        self.device = self.cfg.device\n",
        "        self.is_train = is_train\n",
        "        self.hidden_size = 768\n",
        "        self.mlp = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "\n",
        "    def forward(self, input_ids, encoder_input_ids, encoder_attn_mask, target_ids= None):\n",
        "        t5_encoder_outputs = self.t5_model.encoder(input_ids=input_ids)\n",
        "        tb_model_ouputs = self.encoder_model(input_ids=encoder_input_ids,\n",
        "                                                 attention_mask = encoder_attn_mask)\n",
        "\n",
        "        t5_final_repr = t5_encoder_outputs.last_hidden_state\n",
        "        tb_final_repr = tb_model_ouputs.last_hidden_state\n",
        "\n",
        "        if self.cfg.fusion_mode == \"concat\":\n",
        "            updated_repr = torch.cat((t5_final_repr, tb_final_repr), dim = -1)\n",
        "            updated_repr = self.mlp(updated_repr)\n",
        "\n",
        "        elif self.cfg.fusion_mode == \"sum\":\n",
        "            updated_repr = t5_final_repr + tb_final_repr\n",
        "\n",
        "        else:\n",
        "            updated_repr = t5_encoder_outputs\n",
        "\n",
        "        t5_encoder_outputs['last_hidden_state'] = updated_repr\n",
        "\n",
        "\n",
        "        if self.is_train:\n",
        "            # Forward pass through decoder\n",
        "            decoder_input_ids = target_ids[:, :-1]  # Shift target ids for decoder input\n",
        "            labels = target_ids[:, 1:].clone()  # Shift target ids for labels\n",
        "            labels[labels == self.t5_tokenizer.pad_token_id] = -100  # Ignore pad token positions in loss\n",
        "            outputs = self.t5_model(\n",
        "                encoder_outputs=t5_encoder_outputs,  # Use modified encoder hidden states\n",
        "                decoder_input_ids=decoder_input_ids,\n",
        "                labels=labels,\n",
        "                )\n",
        "        else:\n",
        "            decoder_input_ids = self.t5_tokenizer.encode(\"\", return_tensors='pt')\n",
        "            decoder_input_ids = decoder_input_ids.to(self.device)\n",
        "            outputs = self.t5_model(\n",
        "                encoder_outputs=t5_encoder_outputs,\n",
        "                decoder_input_ids=decoder_input_ids,\n",
        "                )\n",
        "\n",
        "        return outputs, t5_encoder_outputs"
      ],
      "metadata": {
        "id": "SBmUBWHCJDug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper Functions"
      ],
      "metadata": {
        "id": "yWNTRcwhKtVK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))"
      ],
      "metadata": {
        "id": "g8-5Ku2tKwJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "ktQ6Z7GRKxCO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_fn(train_loader, model, optimizer, epoch, device=CFG.device):\n",
        "    # Enabling Model Training Mode\n",
        "    model.train()\n",
        "\n",
        "    losses = AverageMeter()\n",
        "    start = end = time.time()\n",
        "    global_step = 0\n",
        "\n",
        "    for step, batch in enumerate(train_loader):\n",
        "        input_ids, encoder_inputs, encoder_attn_mask, target_ids = batch\n",
        "\n",
        "        input_ids = input_ids.to(device)\n",
        "        encoder_inputs = encoder_inputs.to(device)\n",
        "        encoder_attn_mask = encoder_attn_mask.to(device)\n",
        "        target_ids = target_ids.to(device)\n",
        "\n",
        "        batch_size = input_ids.size(0)\n",
        "\n",
        "        outputs, _ = model(input_ids, encoder_inputs, encoder_attn_mask, target_ids)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n",
        "        losses.update(loss.item(), batch_size)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        end = time.time()\n",
        "\n",
        "        if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n",
        "            print('Epoch: [{0}][{1}/{2}] '\n",
        "                  'Elapsed {remain:s} '\n",
        "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
        "                  'Grad: {grad_norm:.4f}  '\n",
        "                  .format(epoch+1, step, len(train_loader),\n",
        "                          remain=timeSince(start, float(step+1)/len(train_loader)),\n",
        "                          loss=losses,\n",
        "                          grad_norm=grad_norm))\n",
        "\n",
        "    return losses.avg"
      ],
      "metadata": {
        "id": "lVEtzKAhK1z-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def valid_fn(valid_loader, model, epoch, device=CFG.device):\n",
        "    # Enabling Model Training Mode\n",
        "    model.eval()\n",
        "\n",
        "    losses = AverageMeter()\n",
        "    start = end = time.time()\n",
        "    global_step = 0\n",
        "\n",
        "    for step, batch in enumerate(valid_loader):\n",
        "\n",
        "        input_ids, encoder_inputs, encoder_attn_mask, target_ids = batch\n",
        "\n",
        "        input_ids = input_ids.to(device)\n",
        "        encoder_inputs = encoder_inputs.to(device)\n",
        "        encoder_attn_mask = encoder_attn_mask.to(device)\n",
        "        target_ids = target_ids.to(device)\n",
        "\n",
        "        batch_size = input_ids.size(0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs, _ = model(input_ids, encoder_inputs, encoder_attn_mask, target_ids)\n",
        "            loss = outputs.loss\n",
        "\n",
        "        losses.update(loss.item(), batch_size)\n",
        "        end = time.time()\n",
        "\n",
        "        if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):\n",
        "            print('EVAL: [{0}/{1}] '\n",
        "                  'Elapsed {remain:s} '\n",
        "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
        "                  .format(step, len(valid_loader),\n",
        "                          loss=losses,\n",
        "                          remain=timeSince(start, float(step+1)/len(valid_loader))))\n",
        "\n",
        "    return losses.avg"
      ],
      "metadata": {
        "id": "ktHULqLuK6xD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train loop\n",
        "\n",
        "def train_loop():\n",
        "\n",
        "    model = CustomModel(CFG)\n",
        "    model = model.to(CFG.device) # GPU Config\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=CFG.learning_rate, eps=CFG.eps, betas=CFG.betas)\n",
        "\n",
        "    best_loss = 1e4\n",
        "\n",
        "    for epoch in range(CFG.epochs):\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        # train function\n",
        "        avg_loss = train_fn(train_dataloader, model, optimizer, epoch)\n",
        "\n",
        "        # eval function\n",
        "        avg_val_loss = valid_fn(valid_dataloader, model, device)\n",
        "\n",
        "\n",
        "        elapsed = time.time() - start_time\n",
        "\n",
        "        if best_loss > avg_val_loss: # Saving the best model w.r.t the score\n",
        "            best_loss = avg_val_loss\n",
        "            torch.save({'model': model.state_dict(),\n",
        "                        },\n",
        "                        OUTPUT_DIR+f\"{CFG.fusion_mode}_best_loss.pth\")\n",
        "\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    return best_loss"
      ],
      "metadata": {
        "id": "lhMcfKPlK_JX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OUTPUT_DIR = \"./\"\n",
        "best_loss = train_loop()\n",
        "model = CustomModel(CFG)\n",
        "model = model.to(CFG.device) # GPU Config\n",
        "state = torch.load(OUTPUT_DIR+f\"{CFG.fusion_mode}_best_loss.pth\",\n",
        "                       map_location=torch.device('cpu'))\n",
        "\n",
        "model.load_state_dict(state['model'])\n",
        "print(model)"
      ],
      "metadata": {
        "id": "14uc4QalLAX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inferencing"
      ],
      "metadata": {
        "id": "B4IB7FOxLKZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text_fn(test_loader, model, tokenizer, max_length=50, device=CFG.device):\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    generated_texts = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for step, batch in enumerate(test_loader):\n",
        "            # Load data from the test loader\n",
        "            input_ids, encoder_inputs, encoder_attn_mask, _ = batch\n",
        "            input_ids = input_ids.to(device)\n",
        "            encoder_inputs = encoder_inputs.to(device)\n",
        "            encoder_attn_mask = encoder_attn_mask.to(device)\n",
        "\n",
        "#             outputs, t5_encoder_outputs = model(input_ids, encoder_inputs,\n",
        "#                                                 encoder_attn_mask, target_ids)\n",
        "\n",
        "            # Generate text\n",
        "            output_sequences = model.t5_model.generate(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=encoder_attn_mask,\n",
        "                max_length=max_length,\n",
        "                num_beams=4,  # Adjust as needed\n",
        "                early_stopping=True\n",
        "            )\n",
        "\n",
        "            # Decode generated sequences\n",
        "            generated_texts.extend([tokenizer.decode(output, skip_special_tokens=True) for output in output_sequences])\n",
        "\n",
        "    return generated_texts"
      ],
      "metadata": {
        "id": "iAZ8DA4bLG4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_texts = generate_text_fn(test_dataloader, model, CFG.t5_tokenizer)\n",
        "df_test['predictions'] = generated_texts\n",
        "df_test.head()"
      ],
      "metadata": {
        "id": "IlfG8MmkLL7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exp_name = CFG.model_name.replace('/', '-') + \"_\" + CFG.encoder_name.replace('/', '-') + \"_\" + CFG.fusion_mode\n",
        "print(exp_name)\n",
        "df_test.to_csv(f\"./Our_Model_{exp_name}_Predictions.csv\", index = False)"
      ],
      "metadata": {
        "id": "wfEXBjjcLPnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CustomModel(CFG, is_train = False)\n",
        "model = model.to(CFG.device) # GPU Config\n",
        "state = torch.load(OUTPUT_DIR+f\"{CFG.fusion_mode}_best_loss.pth\",\n",
        "                       map_location=torch.device('cpu'))\n",
        "\n",
        "model.load_state_dict(state['model'])"
      ],
      "metadata": {
        "id": "cSJvfq5sLSsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_single_text_fn(input_text, model, device=CFG.device):\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    decoder_input_ids = CFG.t5_tokenizer.encode(\"\", return_tensors='pt')\n",
        "    decoder_input_ids = decoder_input_ids.to(device)\n",
        "\n",
        "    input_ids = CFG.t5_tokenizer.encode(input_text, return_tensors='pt', padding='max_length',\n",
        "                                                truncation=True, max_length=CFG.max_len)\n",
        "\n",
        "    encoder_inputs = CFG.encoder_tokenizer(input_text, return_tensors='pt', padding='max_length',\n",
        "                                                truncation=True, max_length=CFG.max_len)\n",
        "    encoder_ids = encoder_inputs.input_ids\n",
        "    encoder_attn_mask = encoder_inputs.attention_mask\n",
        "\n",
        "    input_ids = input_ids.to(device)\n",
        "    encoder_ids = encoder_ids.to(device)\n",
        "    encoder_attn_mask = encoder_attn_mask.to(device)\n",
        "\n",
        "    outputs, t5_encoder_outputs = model(input_ids, encoder_ids,\n",
        "                                        encoder_attn_mask)\n",
        "\n",
        "    # Generate text\n",
        "    output_sequences = model.t5_model.generate(\n",
        "            input_ids=None,  # The input_ids will be None since we use encoder_outputs directly\n",
        "            encoder_outputs=t5_encoder_outputs,\n",
        "            decoder_input_ids=decoder_input_ids,\n",
        "            max_length=50,  # Set the max length for generated text\n",
        "            num_beams=5,  # Beam search for better quality text\n",
        "            early_stopping=True  # Stop when the end token is generated\n",
        "        )\n",
        "\n",
        "    generated_text = CFG.t5_tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
        "    return generated_text"
      ],
      "metadata": {
        "id": "bU53iR8qLUuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sample Inference"
      ],
      "metadata": {
        "id": "2vUAzxJdLilG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"Ami vaat khai\"\n",
        "text = generate_single_text_fn(input_text, model)\n",
        "text"
      ],
      "metadata": {
        "id": "rfvy1aitLmaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Additional Code"
      ],
      "metadata": {
        "id": "HiD50OyJLodw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Set the model to evaluation mode\n",
        "# model.eval()\n",
        "\n",
        "# # Function to generate text for a test sample\n",
        "# def generate_text(input_text):\n",
        "#     input_ids = tokenizer(input_text, return_tensors='pt').input_ids\n",
        "\n",
        "\n",
        "# t5_encoder_outputs\n",
        "#     # Prepare the decoder input\n",
        "#     decoder_input_ids = tokenizer.encode(\"\", return_tensors='pt')\n",
        "\n",
        "#     # Generate text using the decoder\n",
        "#     outputs = model.generate(\n",
        "#         input_ids=None,  # The input_ids will be None since we use encoder_outputs directly\n",
        "#         encoder_outputs=encoder_outputs,\n",
        "#         decoder_input_ids=decoder_input_ids,\n",
        "#         max_length=50,  # Set the max length for generated text\n",
        "#         num_beams=5,  # Beam search for better quality text\n",
        "#         early_stopping=True  # Stop when the end token is generated\n",
        "#     )\n",
        "\n",
        "#     # Decode the generated tokens to text\n",
        "#     generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "#     return generated_text\n",
        "\n",
        "# # Test input text\n",
        "# test_input_text = \"translate English to French: I love natural language processing.\"\n",
        "\n",
        "# # Generate text for the test sample\n",
        "# generated_output_text = generate_text(test_input_text)\n",
        "# print(\"Generated text:\", generated_output_text)\n",
        "\n",
        "# # For batch wise\n",
        "\n",
        "# from transformers import MarianMTModel, MarianTokenizer\n",
        "\n",
        "# # Load the MarianMTModel for English to French translation\n",
        "# model_name = \"Helsinki-NLP/opus-mt-en-fr\"\n",
        "# tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "# model = MarianMTModel.from_pretrained(model_name)\n",
        "\n",
        "# # Define a list of input sentences in English\n",
        "# input_texts = [\"Hello, how are you?\", \"What is your name?\", \"How was your day?\"]\n",
        "\n",
        "# # Tokenize the input texts\n",
        "# inputs = tokenizer(input_texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "# # Translate the input texts in batch\n",
        "# translated = model.generate(**inputs)\n",
        "\n",
        "# # Decode the translated outputs\n",
        "# translated_texts = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
        "\n",
        "# # Print the translated texts\n",
        "# for input_text, translated_text in zip(input_texts, translated_texts):\n",
        "#     print(\"Input:\", input_text)\n",
        "#     print(\"Translated:\", translated_text)"
      ],
      "metadata": {
        "_uuid": "00ede3ad-30cf-4641-b70b-af13f9b7100a",
        "_cell_guid": "1ba5ca0d-c880-4f2f-933e-2cb33256cd30",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true,
        "id": "QneZRt9fCyWW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}